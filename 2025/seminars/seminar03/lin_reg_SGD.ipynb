{"cells":[{"cell_type":"markdown","metadata":{"id":"fpyFfRGqD_Jv"},"source":["# Seminar 3. Linear regression and SGD"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2020-02-19T18:43:24.015593Z","start_time":"2020-02-19T18:43:23.252434Z"},"scrolled":true,"id":"FYM47hdiD_J2"},"outputs":[],"source":["import numpy as np\n","import matplotlib\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","random_seed = 218\n","\n","matplotlib.rcParams.update({'font.size': 16})"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n-8Uxig-D_J4"},"outputs":[],"source":["n_features = 2\n","n_objects = 100\n","np.random.seed(random_seed)\n","\n","# Let it be the *true* weights vector\n","w_true = np.random.normal(size=(n_features))\n","\n","X = np.random.uniform(-5, 5, (n_objects, n_features))\n","\n","# For different scales of features. In case of 3 features the code is equal to the commented line below\n","# X *= np.array([[1, 3, 5]])\n","X *= (np.arange(n_features) * 2 + 1)[np.newaxis, :]\n","\n","# Here comes the *true* target vector\n","Y = X.dot(w_true) + np.random.normal(0, 1, n_objects)"]},{"cell_type":"markdown","metadata":{"id":"ceGsN3yED_J4"},"source":["*Recap:*\n","In case of linear model\n","$$\n","\\hat{Y} = X\\mathbf{w}\n","$$\n","and __MSE__ loss function\n","$$\n","Q(Y, X, \\mathbf{w}) = MSE(Y, X\\mathbf{w}) =  \\|Y - X\\mathbf{w}\\|^2_2 = \\sum_i (y_i - \\mathbf{x}^T_i \\mathbf{w})^2\n","$$\n","analytical solution takes simple form:\n","\n","$$\n","\\mathbf{w}^* = (X^T X)^{-1}X^T Y.\n","$$\n","\n","Let's check how it works:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MKMaRJGnD_J5"},"outputs":[],"source":["w_star = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(Y)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J3TMhi22D_J5"},"outputs":[],"source":["w_star"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QzzX54KjD_J6"},"outputs":[],"source":["w_true"]},{"cell_type":"markdown","metadata":{"id":"4TjlF-KaD_J6"},"source":["As we can see, the analytical solution is quite close to the original one."]},{"cell_type":"markdown","metadata":{"id":"wpeCJapuD_J6"},"source":["Now let's generate the dataset with correlated features:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jtxYqzDCD_J7"},"outputs":[],"source":["n_features = 3\n","n_objects = 300\n","eps = 1e-3\n","\n","# Let it be the *true* weights vector\n","w_true = np.random.normal(size=(n_features, ))\n","\n","X = np.random.uniform(-5, 5, (n_objects, n_features))\n","\n","# Now we duplicate the second feature with some small noise, so featues 2 and 3 are collinear\n","X[:, -1] = X[:, -2] + np.random.uniform(-eps, eps, X[:, -2].shape)\n","\n","# Here comes the *true* target vector\n","Y = X.dot(w_true) + np.random.normal(0, 1, (n_objects))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eDHb3HjvD_J7"},"outputs":[],"source":["w_star = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(Y)\n","w_star"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CSpGPsXiD_J7"},"outputs":[],"source":["w_true"]},{"cell_type":"markdown","metadata":{"id":"ZsdvswjiD_J8"},"source":["As we can see, the second and third coefficents are opposite. This makes our model highly *unstable*."]},{"cell_type":"markdown","metadata":{"id":"T2hgBNgOD_J8"},"source":["How could one actually fix it? Here comes the __regularization__.\n","\n","Let's use the L2 norm of weigths vector as a regularization term to constrain the desired solution.\n","\n","$$\n","Q_{\\text{reg}}(Y, X, \\mathbf{w}) = MSE(Y, X\\mathbf{w}) + \\lambda\\|\\mathbf{w}\\|_2^2=  \\|Y - X\\mathbf{w}\\|^2_2 + \\lambda\\|\\mathbf{w}\\|^2_2= \\sum_i (y_i - \\mathbf{x}^T_i \\mathbf{w})^2 + \\sum_p w^2_p\n","$$\n","\n","Analytical solution is available in this case as well:\n","\n","$$\n","\\mathbf{w}^*_{\\text{reg}} = (X^T X + \\lambda I_p)^{-1}X^T Y,\n","$$\n","where $I_p$ is diagonal matrix consisting of 1s (with size p).\n","\n","__Be careful with the regularization term if you have included the column of 1s into X matrix! We do not want regularize the bias (free) term in our linear model.__"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JxK_IPA_D_J9"},"outputs":[],"source":["reg = 1e-0\n","w_star_reg = np.linalg.inv(X.T.dot(X) + reg*np.eye(n_features)).dot(X.T).dot(Y)\n","w_star_reg"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BYAsb68ND_J9"},"outputs":[],"source":["w_true"]},{"cell_type":"markdown","metadata":{"id":"W41K0FIpD_J-"},"source":["### Gradient descent\n","\n","The analytical solution described above includes invertion of the matrix $X^T X$ (or $X^T X + \\lambda I$), which is quite expensive in terms of computation resourses. The complexity of matrix inversion can be estimated as $O(p^3 + p^2 N)$. This leads us to the iterative optimization methods, which are more efficient and are de-facto the main approach to optimization in Machine Learning.\n","\n","Gradient descent is one of the most popular optimization methods. Worth to mention the fact that the minimization (maximization) target (e.g loss function value) should be differentiable w.r.t model parameters. Using the gradient descent, the weights vector $\\mathbf{w}^{(t+1)}$ on step $t+1$ can be expressed in the following form:\n","$$\n","\\mathbf{w}^{(t+1)} = \\mathbf{w}^{(t)} - \\eta_t \\nabla Q(\\mathbf{w}^{(t)}),\n","$$\n","where $\\eta_t$ stays for the gradient step (usually referred as _learning rate_).\n","\n","The gradient in case of MSE loss function takes the following form:\n","\n","$$\n","\\nabla Q(\\mathbf{w}) = -2X^TY + 2X^TX\\mathbf{w} = 2X^T(X\\mathbf{w} - Y).\n","$$\n","\n","In this case the complexity is only $O(pN)$. To make it even more effective (and using the hypothesis of homogeneous data in the dataset) one could use _stochastic gradient descent_, which computes the gradient only over some random subset of data K points, so the final complexity decreases to $O(pK)$, where $K << N$."]},{"cell_type":"markdown","metadata":{"id":"v0O19hPmD_J-"},"source":["### Visuailizing the gradient descent trajectory\n","This part is deeply based on [Evgeny Sokolov](https://github.com/esokolov) open materials.\n","\n","Let's take a close look on the optimization path in simple two-dimentional space (where features are in different scales). We will use MSE loss function."]},{"cell_type":"markdown","metadata":{"id":"jAN852BmD_J_"},"source":["The plots below show $\\mathbf{w}^{(t)}$ values on every step $t$. The red dot in the center stays for $\\mathbf{w}_{\\text{true}}$."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-hgsTTuqD_J_"},"outputs":[],"source":["random_seed = 43\n","n_features = 2\n","n_objects = 300\n","num_steps = 43\n","\n","# Let it be the *true* weights vector\n","w_true = np.random.normal(size=(n_features, ))\n","\n","X = np.random.uniform(-5, 5, (n_objects, n_features))\n","\n","# For different scales of features. In case of 3 features the code is equal to the commented line below\n","# X *= np.array([[1, 3, 5]])\n","X *= (np.arange(n_features) * 2 + 1)[np.newaxis, :]\n","\n","# Here comes the *true* target vector\n","Y = X.dot(w_true) + np.random.normal(0, 1, n_objects)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CrpEy7_bD_J_"},"outputs":[],"source":["np.random.seed(random_seed)\n","w_0 = np.random.uniform(-2, 2, n_features)-0.5\n","w = w_0.copy()\n","w_list = [w.copy()]\n","step_size = 1e-2\n","\n","for i in range(num_steps):\n","    w -= # YOUR CODE HERE\n","    w_list.append(w.copy())\n","w_list = np.array(w_list)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B-qZco10D_KA"},"outputs":[],"source":["# compute level set\n","A, B = np.meshgrid(np.linspace(-2, 2, 100), np.linspace(-2, 2, 100))\n","\n","levels = np.empty_like(A)\n","for i in range(A.shape[0]):\n","    for j in range(A.shape[1]):\n","        w_tmp = np.array([A[i, j], B[i, j]])\n","        levels[i, j] = np.mean(np.power(np.dot(X, w_tmp) - Y, 2))\n","\n","plt.figure(figsize=(13, 9))\n","plt.title('GD trajectory')\n","plt.xlabel('$w_1$')\n","plt.ylabel('$w_2$')\n","plt.xlim(w_list[:, 0].min() - 0.1, w_list[:, 0].max() + 0.1)\n","plt.ylim(w_list[:, 1].min() - 0.1, w_list[:, 1].max() + 0.1)\n","plt.gca().set_aspect('equal')\n","\n","# visualize the level set\n","CS = plt.contour(A, B, levels, levels=np.logspace(0, 2, num=15), cmap=plt.cm.rainbow_r)\n","CB = plt.colorbar(CS, shrink=0.8, extend='both')\n","\n","# visualize trajectory\n","plt.scatter(w_true[0], w_true[1], c='r')\n","plt.scatter(w_list[:, 0], w_list[:, 1])\n","plt.plot(w_list[:, 0], w_list[:, 1])\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"vP2esa2HD_KA"},"source":["The gradient vector is orthogonal to the equipotential surface . That's the reason why the optimization path is not so smooth. Let's visualize the gradient directions to make it more clear."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-xy2XRFLD_KB"},"outputs":[],"source":["# compute level set\n","A, B = np.meshgrid(np.linspace(-3, 3, 100), np.linspace(-3, 3, 100))\n","A_mini, B_mini = np.meshgrid(np.linspace(-3, 3, 40), np.linspace(-3, 3, 40))\n","\n","levels = np.empty_like(A)\n","for i in range(A.shape[0]):\n","    for j in range(A.shape[1]):\n","        w_tmp = np.array([A[i, j], B[i, j]])\n","        levels[i, j] = np.mean(np.power(np.dot(X, w_tmp) - Y, 2))\n","\n","# visualize the level set\n","plt.figure(figsize=(13, 9))\n","CS = plt.contour(A, B, levels, levels=np.logspace(-1, 1.5, num=40), cmap=plt.cm.rainbow_r)\n","CB = plt.colorbar(CS, shrink=0.8, extend='both')\n","\n","# visualize the gradients\n","gradients = np.empty_like(A_mini)\n","for i in range(A_mini.shape[0]):\n","    for j in range(A_mini.shape[1]):\n","        w_tmp = np.array([A_mini[i, j], B_mini[i, j]])\n","        antigrad = - 2 * 1e-3 * np.dot(X.T, np.dot(X, w_tmp) - Y) / Y.shape[0]\n","        plt.arrow(A_mini[i, j], B_mini[i, j], antigrad[0], antigrad[1], head_width=0.02)\n","\n","plt.title('Antigradients demonstration')\n","plt.xlabel(r'$w_1$')\n","plt.ylabel(r'$w_2$')\n","plt.xlim((w_true[0] - 1.5, w_true[0] + 1.5))\n","plt.ylim((w_true[1] - .5, w_true[1] + .5))\n","plt.gca().set_aspect('equal')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"_LLHTIUyD_KB"},"source":["Now let's take a look at the _stochastic gradient descent_. Let the number of elements the loss function computed on each state (`batch_size`) be equal to $10$."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uCEAZJYHD_KB"},"outputs":[],"source":["np.random.seed(random_seed)\n","batch_size = 10\n","w = w_0.copy()\n","w_history_list = [w.copy()]\n","lr = 1e-2\n","\n","for i in range(num_steps):\n","    sample_indices = # YOUR CODE HERE\n","    w -= # YOUR CODE HERE\n","    w_history_list.append(w.copy())\n","w_history_list = np.array(w_history_list)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s5RFKYojD_KC"},"outputs":[],"source":["# compute level set\n","A, B = np.meshgrid(np.linspace(-2, 2, 100), np.linspace(-2, 2, 100))\n","\n","levels = np.empty_like(A)\n","for i in range(A.shape[0]):\n","    for j in range(A.shape[1]):\n","        w_tmp = np.array([A[i, j], B[i, j]])\n","        levels[i, j] = np.mean(np.power(np.dot(X, w_tmp) - Y, 2))\n","\n","\n","plt.figure(figsize=(13, 9))\n","plt.title('SGD trajectory')\n","plt.xlabel(r'$w_1$')\n","plt.ylabel(r'$w_2$')\n","plt.xlim((w_history_list[:, 0].min() - 0.1, w_history_list[:, 0].max() + 0.1))\n","plt.ylim((w_history_list[:, 1].min() - 0.1, w_history_list[:, 1].max() + 0.1))\n","plt.gca().set_aspect('equal')\n","\n","# visualize the level set\n","CS = plt.contour(A, B, levels, levels=np.logspace(0, 2, num=30), cmap=plt.cm.rainbow_r)\n","CB = plt.colorbar(CS, shrink=0.8, extend='both')\n","\n","# visualize trajectory\n","plt.scatter(w_true[0], w_true[1], c='r')\n","plt.scatter(w_history_list[:, 0], w_history_list[:, 1])\n","plt.plot(w_history_list[:, 0], w_history_list[:, 1])\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"6C5U5GHHD_KC"},"source":["As you can see from the plot, SGD \"wanders\" around the optima. It is controlled by the SGD step size $\\eta_k$ and the convergence is not guaranteed in general case. For SGD method convergence given the sequence of steps $\\{\\eta_k\\}$ it is necessary that [Robbins-Monroe Conditions](https://projecteuclid.org/download/pdf_1/euclid.aoms/1177729586) are satisfied:\n","$$\n","\\sum_{k = 1}^\\infty \\eta_k = \\infty, \\qquad \\sum_{k = 1}^\\infty \\eta_k^2 < \\infty.\n","$$\n","More intuitively, those conditions may be explained as follows:\n","1. A sequence of steps $\\{\\eta_k\\}$ should diverge, so optimization method is capable or reaching any point in the given parameter space,\n","2. At the same time it should diverge \"not so fast\""]},{"cell_type":"markdown","metadata":{"id":"9cH5bLVDD_KD"},"source":["Let's analyze SGD trajectories, which are generated by a sequence of steps, satisfying the Robbins-Monroe Conditions:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wzSoFQ1BD_KD"},"outputs":[],"source":["np.random.seed(random_seed)\n","w = w_0.copy()\n","w_list = [w.copy()]\n","lr_0 = 0.02\n","\n","for i in range(num_steps):\n","    lr = lr_0 / ((i+1) ** # What should the power be? )\n","    sample_indices = # YOUR CODE HERE\n","    w -= # YOUR CODE HERE\n","    w_list.append(w.copy())\n","w_list = np.array(w_list)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CVnNyzyYD_KD"},"outputs":[],"source":["# compute level set\n","A, B = np.meshgrid(np.linspace(-2, 2, 100), np.linspace(-2, 2, 100))\n","\n","levels = np.empty_like(A)\n","for i in range(A.shape[0]):\n","    for j in range(A.shape[1]):\n","        w_tmp = np.array([A[i, j], B[i, j]])\n","        levels[i, j] = np.mean(np.power(np.dot(X, w_tmp) - Y, 2))\n","\n","plt.figure(figsize=(13, 9))\n","plt.title('SGD trajectory')\n","plt.xlabel(r'$w_1$')\n","plt.ylabel(r'$w_2$')\n","plt.xlim((w_list[:, 0].min() - 0.1, w_list[:, 0].max() + 0.1))\n","plt.ylim((w_list[:, 1].min() - 0.1, w_list[:, 1].max() + 0.1))\n","plt.gca().set_aspect('equal')\n","\n","# visualize the level set\n","CS = plt.contour(A, B, levels, levels=np.logspace(0, 2, num=40), cmap=plt.cm.rainbow_r)\n","CB = plt.colorbar(CS, shrink=0.8, extend='both')\n","\n","# visualize trajectory\n","plt.scatter(w_true[0], w_true[1], c='r')\n","plt.scatter(w_list[:, 0], w_list[:, 1])\n","plt.plot(w_list[:, 0], w_list[:, 1])\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"fto7_Vv0D_KE"},"source":["### Comparing the convergence speed\n","Finally, it is important to compare the convergence speed for full and stochastic GD. Let's generate a random dataset and plot the loss function value w.r.t. iteration number"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ASJyELOAD_KF"},"outputs":[],"source":["# data generation\n","n_features = 50\n","n_objects = 1000\n","num_steps = 500\n","batch_size = 10\n","\n","w_true = np.random.uniform(-2, 2, n_features)\n","\n","X = np.random.uniform(-10, 10, (n_objects, n_features))\n","Y = X.dot(w_true) + np.random.normal(0, 5, n_objects)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BhvcU_q5D_KG"},"outputs":[],"source":["lr_sgd = 1e-3\n","lr_gd = 1e-3\n","w_sgd = np.random.uniform(-4, 4, n_features)\n","w_gd = w_sgd.copy()\n","residuals_sgd = [np.mean(np.power(np.dot(X, w_sgd) - Y, 2))]\n","residuals_gd = [np.mean(np.power(np.dot(X, w_gd) - Y, 2))]\n","\n","for i in range(num_steps):\n","    lr = lr_sgd / ((i+1) ** 0.51)\n","    sample = np.random.randint(n_objects, size=batch_size)\n","    w_sgd -= 2 * lr * np.dot(X[sample].T, np.dot(X[sample], w_sgd) - Y[sample]) / batch_size\n","    residuals_sgd.append(np.mean(np.power(np.dot(X, w_sgd) - Y, 2)))\n","\n","    w_gd -= 2 * lr_gd * np.dot(X.T, np.dot(X, w_gd) - Y) / Y.shape[0]\n","    residuals_gd.append(np.mean(np.power(np.dot(X, w_gd) - Y, 2)))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3mTGMAt2D_KI"},"outputs":[],"source":["plt.figure(figsize=(13, 6))\n","plt.plot(range(num_steps+1), residuals_gd, label='Full GD')\n","plt.plot(range(num_steps+1), residuals_sgd, label='SGD')\n","plt.title('Empirial risk over iterations')\n","plt.xlim((-1, num_steps+1))\n","plt.legend()\n","plt.xlabel('Iter num')\n","plt.ylabel(r'Q($w$)')\n","plt.grid()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"KyFE1jcCD_KI"},"source":["As demonstrated, GD reaches optima within several iterations, while SGD may appear less stable and requires more time to converge. Usually larger models demonstrate larger fluctuations for loss function values during the convergence process of stochastic gradient-based methods. In practice, SGD step size may be adjusted to achieve better convergence speed and there are several methods which implement adaptive gradient descent step size: AdaGrad, Adam, RMSProp etc."]},{"cell_type":"markdown","metadata":{"id":"dWP74ZZsD_KJ"},"source":["## Bonus part. Own Linear Regression"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2020-02-19T18:48:24.750053Z","start_time":"2020-02-19T18:48:24.746577Z"},"id":"gYBhfu6SD_KJ"},"outputs":[],"source":["from sklearn.base import BaseEstimator, RegressorMixin\n","# also ClassifierMixin and TransformerMixin exist"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2020-02-19T18:50:28.936505Z","start_time":"2020-02-19T18:50:28.927717Z"},"id":"Zz0LtaAPD_KJ"},"outputs":[],"source":["class LinearRegressionSGD(BaseEstimator, RegressorMixin):\n","    '''LinearRegression with L2 regularization and SGD optimizer\n","    '''\n","    def __init__(\n","        self, C: float=1.0,\n","        batch_size: int=25,\n","        lr: float=1e-2,\n","        num_steps: int=200,\n","    ) -> None:\n","        self.C = C\n","        self.batch_size = batch_size\n","        self.lr = lr\n","        self.num_steps = num_steps\n","\n","    def fit(self, X, Y):\n","        w = np.random.randn(X.shape[1])[:, None]\n","        n_objects = len(X)\n","\n","        # this is just copied from above\n","        for i in range(self.num_steps):\n","            sample_indices = np.random.randint(n_objects, size=self.batch_size)\n","            w -= 2 * self.lr * np.dot(X[sample_indices].T, np.dot(X[sample_indices], w) - Y[sample_indices]) / self.batch_size\n","\n","        self.w = w\n","        return self\n","\n","    def predict(self, X):\n","        return X@self.w"]},{"cell_type":"markdown","metadata":{"id":"hRrmeeEUD_KK"},"source":["Let's generate dataset with differently scaled features"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2020-02-19T18:50:42.390314Z","start_time":"2020-02-19T18:50:40.904780Z"},"id":"w2HI_avtD_KL"},"outputs":[],"source":["n_features = 700\n","n_objects = 100000\n","num_steps = 150\n","\n","w_true = np.random.uniform(-2, 2, (n_features, 1))\n","\n","X = np.random.uniform(-100, 100, (n_objects, n_features)) * np.arange(n_features)\n","Y = X.dot(w_true) + np.random.normal(0, 10, (n_objects, 1))"]},{"cell_type":"markdown","metadata":{"id":"kyxuNQxXD_KM"},"source":["and split it to train and test"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2020-02-19T18:50:43.599348Z","start_time":"2020-02-19T18:50:43.596610Z"},"id":"OV7gLqHPD_KM"},"outputs":[],"source":["from sklearn.model_selection import train_test_split"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2020-02-19T18:55:10.577345Z","start_time":"2020-02-19T18:55:10.180458Z"},"id":"PfEqr1NUD_KM"},"outputs":[],"source":["x_train, x_test, y_train, y_test = train_test_split(X, Y)"]},{"cell_type":"markdown","metadata":{"id":"fRNLmEAHD_KM"},"source":["Now let's test our solution"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2020-02-19T18:56:42.186020Z","start_time":"2020-02-19T18:56:42.118206Z"},"id":"XZJwBcaBD_KN"},"outputs":[],"source":["own_lr = LinearRegressionSGD().fit(x_train, y_train)\n","print(f'R2: {own_lr.score(x_test, y_test)}')"]},{"cell_type":"markdown","metadata":{"id":"jZYuZS8nD_KN"},"source":["OOOOOOOOOPS!!!\n","\n","Something went wrong. What could it be?\n","\n","During our SGD we've encountered too big values to store in float.\n","\n","That leads us to feature normalization.\n","Lest's scale features: just subtract mean from each feature and divide by sample variation"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2020-02-19T19:00:31.413133Z","start_time":"2020-02-19T19:00:31.410170Z"},"id":"UdVBJTe8D_KN"},"outputs":[],"source":["from sklearn.preprocessing import StandardScaler"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2020-02-19T19:03:03.598988Z","start_time":"2020-02-19T19:03:01.932987Z"},"id":"M2PQfZVYD_KN"},"outputs":[],"source":["scaler = StandardScaler()\n","scaler.fit(x_train)\n","x_scaled = scaler.transform(x_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2020-02-19T19:03:22.671680Z","start_time":"2020-02-19T19:03:22.643142Z"},"id":"Vh0rY9myD_KN"},"outputs":[],"source":["own_lr = LinearRegressionSGD().fit(x_scaled, y_train)"]},{"cell_type":"markdown","metadata":{"id":"mDxRq0p5D_KO"},"source":["But for test we need to scale test features"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2020-02-19T19:04:02.064690Z","start_time":"2020-02-19T19:04:01.902071Z"},"id":"vg6Z1gmTD_KO"},"outputs":[],"source":["x_test_scaled = scaler.transform(x_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2020-02-19T19:04:09.395759Z","start_time":"2020-02-19T19:04:09.383991Z"},"id":"rpAn_5_aD_KO"},"outputs":[],"source":["print(f'R2: {own_lr.score(x_test_scaled, y_test)}')"]},{"cell_type":"markdown","metadata":{"id":"CINGlHx_D_KP"},"source":["Wow! we didn't implement no `score` method. But `sklearn`'s base class provide us it aleready implemented.\n","\n","You note that scaling data before prediction is not a big pleasure. So we could get rid of this bulkiness with pipelines"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2020-02-19T19:08:17.547462Z","start_time":"2020-02-19T19:08:17.538028Z"},"id":"i6NvljcyD_KP"},"outputs":[],"source":["from sklearn.pipeline import make_pipeline"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2020-02-19T19:08:22.920876Z","start_time":"2020-02-19T19:08:22.917665Z"},"id":"5LEHB000D_KP"},"outputs":[],"source":["pipe = make_pipeline(\n","    StandardScaler(),\n","    LinearRegressionSGD(),\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2020-02-19T19:08:54.244228Z","start_time":"2020-02-19T19:08:52.345667Z"},"id":"geNRWiRbD_KQ"},"outputs":[],"source":["pipe.fit(x_train, y_train)\n","print(f'R2: {pipe.score(x_test, y_test)}')"]},{"cell_type":"markdown","metadata":{"id":"OLv3xWSmD_KQ"},"source":["As if we don't have any complex assembly behind pipeline interface!\n","\n","And no data leak guaranteed as a gift!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aOVckY9ID_KQ"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"pyoadfe","language":"python","name":"pyoadfe"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}