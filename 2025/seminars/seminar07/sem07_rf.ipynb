{"cells":[{"cell_type":"markdown","source":["# Семинар 7: Случайный лес\n","\n","[Источник](https://github.com/hse-ds/iad-intro-ds/tree/master/2023/seminars/sem13_rf)"],"metadata":{"id":"rJRW1jPZVGgj"},"id":"rJRW1jPZVGgj"},{"cell_type":"markdown","metadata":{"id":"72wqiWwaPz7F"},"source":["## 1. Bias-Variance decomposition\n","\n","Вспомним, что функцию потерь в задачах регрессии или классификации можно разложить на три компоненты: смещение (bias), дисперсию (variance) и шум (noise). Эти компоненты позволяют описать сложность алгоритма, альтернативно сравнению ошибок на тренировочной и тестовой выборках. Хотя такое разложение можно построить для произвольной функции потерь, наиболее просто (и классически) оно строится для среднеквадратичной функции в задаче регрессии, что мы и рассмотрим ниже.\n","\n","Пусть $(X, y)$ — некоторая выборка. Обучим интересующий нас алгоритм на этой выборке и сделаем предсказания на ней. Обозначим предсказания как $\\hat{y}$. Тогда\n","\n","$$\n","\\mathrm{bias} := \\mathbb{E}(\\hat{y}) - y\n","$$\n","\n","$$\n","\\mathrm{variance} := \\mathbb{E}[\\mathbb{E}(\\hat{y}) - \\hat{y}]^2\n","$$\n","\n","$$\n","\\mathrm{noise} := \\mathbb{E}[y - \\mathbb{E}(y)]^2\n","$$\n","\n","Ожидаемую среднеквадратичную ошибку на тренировочной выборке можно разложить как\n","\n","$$\n","\\mathrm{E}[y - \\hat{y}]^2 = \\mathrm{bias}^2 + \\mathrm{variance} + \\mathrm{noise}.\n","$$\n","\n","**Задание для самых смелых:** покажите, что это разложение корректно. Проверьте себя [здесь](https://github.com/esokolov/ml-course-hse/blob/master/2020-fall/lecture-notes/lecture08-ensembles.pdf).\n","\n","**Техническое замечание:** все математические ожидания в разложении выше берутся по объектам тренировочной выборки, то есть это разложение верно для среднеквадратичной ошибки на тренировочной выборке, которую иногда называют MSE for estimator. Тем не менее нам интересна и величина ошибки на ненаблюдаемых данных, которую иногда называют MSE for predictor. В этом случае математическое ожидание ошибки следует брать по ненаблюдаемым объектам. Для решения этой проблемы зачастую предполагается, что тренировочная и тестовая выборка имеют одинаковое распределение, и математическое ожидание берётся по всевозможным вариациям тренировочной выборки. Суть разложения при этом не изменится, однако запись его станет более громоздкой. Посмотреть на это можно [здесь](https://medium.com/data-science/mse-and-bias-variance-decomposition-77449dd2ff55).\n","\n","Заметим, что так как на практике мы считаем оценки математических ожиданий и зачастую имеем доступ к тестовой выборке, то проблем с расчётом **оценок** MSE for estimator и MSE for predictor не возникает.\n","\n","Разберёмся с интерпретацией компонент:\n","- $\\mathrm{Bias}$ — отклонение среднего ответа алгоритма от ответа идеального алгоритма. $\\mathrm{Bias}$ отражает ошибку модели, возникающую из-за простоты модели. Высокое смещение обычно является показателем того, что модель недообучена.\n","- $\\mathrm{Variance}$ — разброс ответов алгоритмов относительно среднего ответа алгоритма. Показывает, насколько сильно небольшие изменения в обучающей выборке скажутся на предсказаниях алгоритма. $\\mathrm{Variance}$ отражает ошибку модели, возникающую из-за чрезмерной сложности модели. Высокая дисперсия обычно является показателем того, что модель переобучена.\n","- $\\mathrm{Noise}$ — ошибка идеального классификатора, естественный неустранимый шум в данных.\n","\n","![](https://i.sstatic.net/r7QFy.png)\n","\n","Посмотрим наглядно на примере полиномиальной регрессии."],"id":"72wqiWwaPz7F"},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2023-05-10T11:49:31.201889Z","start_time":"2023-05-10T11:49:30.519955Z"},"id":"Cz32Zm7xPz7I"},"outputs":[],"source":["import warnings\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","from sklearn.datasets import fetch_california_housing\n","from sklearn.linear_model import LinearRegression\n","from sklearn.model_selection import train_test_split\n","from tqdm.auto import trange\n","\n","warnings.filterwarnings(\"ignore\")"],"id":"Cz32Zm7xPz7I"},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2023-05-10T11:49:31.338511Z","start_time":"2023-05-10T11:49:31.205037Z"},"id":"kqbSRlgDPz7J"},"outputs":[],"source":["# Выборка\n","np.random.seed(42)\n","N = 10\n","X = np.linspace(-5, 5, N).reshape(-1, 1)\n","y = np.sin(X) + np.random.normal(0, 0.2, size=N).reshape(-1, 1)\n","\n","X_test = np.linspace(-5, 5, N // 2).reshape(-1, 1)\n","y_test = np.sin(X_test) + np.random.normal(0, 0.2, size=N // 2).reshape(-1, 1)\n","\n","# Очень простая модель (регрессия на константу)\n","too_simple_model_predictions = np.mean(y) * np.ones_like(y)\n","\n","# В меру сложная модель\n","X_ok = np.hstack([X, X**2, X**3])\n","ok_model = LinearRegression()\n","ok_model.fit(X_ok, y)\n","ok_model_predictions = ok_model.predict(X_ok)\n","\n","# Очень сложная модель\n","X_compl = np.hstack(\n","    [X, X**2, X**3, X**4, X**5, X**6, X**7, X**8, X**9, X**10]\n",")\n","compl_model = LinearRegression()\n","compl_model.fit(X_compl, y)\n","compl_model_predictions = compl_model.predict(X_compl)\n","\n","plt.figure(figsize=(10, 7))\n","\n","plt.title(\"Сравнение моделей разной сложности\")\n","plt.scatter(X, y, label=\"Тренировочная выборка\")\n","plt.scatter(X_test, y_test, c=\"r\", label=\"Тестовая выборка\")\n","plt.plot(X, too_simple_model_predictions, label=\"Очень простая модель\")\n","plt.plot(X, ok_model_predictions, label=\"В меру сложная модель\")\n","plt.plot(X, compl_model_predictions, label=\"Очень сложная модель\")\n","plt.grid()\n","plt.legend();"],"id":"kqbSRlgDPz7J"},{"cell_type":"markdown","metadata":{"id":"25gGI33dPz7K"},"source":["**Задание:** пользуясь определениями выше, прокомментируйте поведение моделей:\n","\n","-\n","-\n","-\n","\n","**Задание:** прокомментируйте величину смещения и дисперсии для следующих моделей:\n","\n","- Линейная регрессия, обучаемая на большой выборке без выбросов и линейно зависимых признаков, в которой признаки сильно коррелируют с целевой переменной.\n","- Решающее дерево, которое строится до тех пор, пока в листах не окажется по одному объекту.\n","- Логистическая регрессия, относящая все точки к одному классу."],"id":"25gGI33dPz7K"},{"cell_type":"markdown","metadata":{"id":"QvFva427Pz7L"},"source":["### Bias-Variance tradeoff\n","\n","Из описания выше можно заметить, что при обучении моделей возникает выбор между смещением и дисперсией: недообученная модель имеет низкую дисперсию, но высокое смещение, а переобученная — низкое смещение, но высокую дисперсию. Этот выбор можно отобразить на картинке ([источник](https://www.bradyneal.com/bias-variance-tradeoff-textbooks-update)). Здесь Total Error – ошибка на тестовой выборке (generalization error).\n","\n","![](https://www.bradyneal.com/img/bias-variance/fortmann-roe-bias-variance.png)\n","\n","Вывод из неё очевиден: строить следует оптимальные по сложности модели.\n","\n","Возникает ли такой выбор при обучении любой модели? Последние исследования показывают, что поведение ошибки при обучении некоторых (современных) моделей не соответствует такой U-образной форме. Например, было показано, что ошибка на тестовой выборке продолжает убывать при расширении (увеличении числа слоёв) нейронных сетей:\n","\n","<img src=\"https://www.bradyneal.com/img/bias-variance/neyshabur.jpg\" alt=\"drawing\" width=\"400\"/>\n","\n","В таких моделях поведение ошибки приобретает сложный вид:\n","\n","\n","<img src=\"https://www.bradyneal.com/img/bias-variance/double_descent.jpg\" alt=\"drawing\" width=\"800\"/>"],"id":"QvFva427Pz7L"},{"cell_type":"markdown","metadata":{"id":"-HvjBFd9Pz7L"},"source":["## 2. От деревьев к случайному лесу\n","\n","### 2.1. Решающее дерево\n","\n","Мотивацию построения алгоритма случайного леса (Random Forest) удобно рассматривать в терминах смещения и дисперсии. Начнём с построения решающего дерева."],"id":"-HvjBFd9Pz7L"},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2023-05-10T11:49:31.346252Z","start_time":"2023-05-10T11:49:31.338141Z"},"id":"zTNYbPKiPz7L"},"outputs":[],"source":["california = fetch_california_housing()\n","california_X = pd.DataFrame(data=california.data, columns=california.feature_names)\n","california_Y = california.target\n","print(f\"X shape: {california_X.shape}, Y shape: {california_Y.shape}\")\n","X_train, X_test, y_train, y_test = train_test_split(\n","    california_X, california_Y, test_size=0.3, shuffle=True\n",")"],"id":"zTNYbPKiPz7L"},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2023-05-10T11:49:31.459233Z","start_time":"2023-05-10T11:49:31.346360Z"},"id":"TmZGMlbzPz7M"},"outputs":[],"source":["from sklearn.metrics import mean_squared_error\n","from sklearn.tree import DecisionTreeRegressor\n","\n","# TODO: обучите решающее дерево без ограничений на тренировочной выборке\n","\n","# TODO: рассчитайте MSE на тренировочной и тестовой выборках\n","\n"],"id":"TmZGMlbzPz7M"},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2023-05-10T11:49:47.314276Z","start_time":"2023-05-10T11:49:31.461093Z"},"id":"MBNE6Tu8Pz7M"},"outputs":[],"source":["from mlxtend.evaluate import bias_variance_decomp\n","\n","# TODO: выведите среднее смещение и среднюю дисперсию модели на тестовой выборке\n","\n"],"id":"MBNE6Tu8Pz7M"},{"cell_type":"markdown","metadata":{"id":"gGPVdaffPz7M"},"source":["Как мы обсуждали на предыдущем семинаре, такое дерево окажется сильно переобученным (высокая дисперсия и низкое смещение). Постараемся исправить это. На лекции мы обсуждали, что один из способов борьбы с переобучением – построение композиций моделей. На этом семинаре мы рассмотрим построение композиций при помощи бэггинга."],"id":"gGPVdaffPz7M"},{"cell_type":"markdown","metadata":{"id":"Ealv8XGmPz7M"},"source":["### 2.2. Бэггинг\n","\n","Вспомним суть алгоритма:\n","1. Обучаем много деревьев на бутстрапированных подвыборках исходной выборки независимо друг от друга. Бутстрапированную подвыборку строим при помощи выбора $N$ (размер исходной выборки) наблюдений из исходной выборки с возвращением.\n","2. Усредняем предсказания всех моделей (например, берём арифметическое среднее).\n","\n","Можно показать, что модель, построенная при помощи бэггинга, будет иметь **то же смещение**, что и у отдельных деревьев, но значительно **меньшую дисперсию** (при выполнении некоторых условий)."],"id":"Ealv8XGmPz7M"},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2023-05-10T11:50:53.873857Z","start_time":"2023-05-10T11:49:47.316082Z"},"id":"Z26BBJ-yPz7N"},"outputs":[],"source":["from sklearn.ensemble import BaggingRegressor\n","\n","base_tree = DecisionTreeRegressor()\n","\n","# TODO: обучите бэггинг с 20 деревьями, каждое из которых строится без ограничений\n","\n","\n","# TODO: рассчитайте MSE на тренировочной и тестовой выборках\n","\n","\n","# TODO: выведите среднее смещение и среднюю дисперсию модели на тестовой выборке\n","\n"],"id":"Z26BBJ-yPz7N"},{"cell_type":"markdown","metadata":{"id":"kDEhwVFmPz7N"},"source":["Как мы видим, по сравнению с единичным деревом смещение практически не изменилось, но дисперсия уменьшилась в несколько раз! Среднеквадратичная ошибка на тренировочной выборке больше не равна 0, а на тестовой — уменьшилась, что говорит о том, что мы успешно победили переобучение единичного решающего дерева.\n","\n","Можем ли мы снизить переобучение ещё сильнее? Можем!"],"id":"kDEhwVFmPz7N"},{"cell_type":"markdown","metadata":{"id":"EMAsq9VPPz7N"},"source":["### 2.3. Случайный лес\n","\n","При построении каждого дерева в бэггинге в ходе создания очередного узла будем выбирать случайный набор признаков, на основе которых производится разбиение. В результате такой процедуры мы уменьшим корреляцию между деревьями, за счёт чего снизим дисперсию итоговой модели. Такой алгоритм назвывается **случайным лесом** (Random Forest).\n","\n","По сравнению с единичным деревом к параметрам случайного леса добавляются:\n","- `max_features` – число признаков, на основе которых проводятся разбиения при построении дерева.\n","\n","- `n_estimators` – число деревьев.\n","\n","Естественно, все параметры, относящиеся к единичному дереву, сохраняются для случайного леса."],"id":"EMAsq9VPPz7N"},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2023-05-10T11:51:54.113083Z","start_time":"2023-05-10T11:50:53.876014Z"},"id":"ZAIAdXh9Pz7N"},"outputs":[],"source":["from sklearn.ensemble import RandomForestRegressor\n","\n","# TODO: обучите случайный лес с 20 деревьями, каждое из которых строится без ограничений\n","\n","\n","# TODO: рассчитайте MSE на тренировочной и тестовой выборках\n","\n","\n","# TODO: выведите среднее смещение и среднюю дисперсию модели на тестовой выборке\n","\n"],"id":"ZAIAdXh9Pz7N"},{"cell_type":"markdown","metadata":{"id":"4ZN1ozSaPz7O"},"source":["Как мы видим, по сравнению с бэггингом смещение вновь осталось практически неизменным, дисперсия немного изменилась. Конечно, если подобрать хорошие гиперпараметры, то получится снизить дисперсию ещё больше.\n","\n","Ошибка на тренировочной выборке увеличилась, а на тестовой — уменьшилась, что означает, что мы добились нашей цели в борьбе с переобученными деревьями!"],"id":"4ZN1ozSaPz7O"},{"cell_type":"markdown","metadata":{"id":"652aqEL0Pz7O"},"source":["## 3. Особенности случайного леса\n","\n","### 3.1. Число деревьев и \"Случайный лес не переобучается\"\n","\n","В своём [блоге](https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#remarks) Лео Бриман (Leo Breiman), создатель случайного леса, написал следующее:\n","\n","> Random forest does not overfit. You can run as many trees as you want.\n","\n","**Обратите внимание:** как говорилось на лекции, случайный лес не переобучается именно с ростом числа деревьев (за счёт совместной работы бэггинга и использования случайных подпространств), но не в принципе. Посмотрим на поведение случайного леса при росте числа деревьев."],"id":"652aqEL0Pz7O"},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2023-05-10T11:52:35.735267Z","start_time":"2023-05-10T11:51:54.114270Z"},"id":"ZhIv39slPz7O"},"outputs":[],"source":["n_trees = 100\n","train_loss = []\n","test_loss = []\n","\n","for i in trange(1, n_trees, 2):\n","    rf = RandomForestRegressor(n_estimators=i, n_jobs=4)\n","    rf.fit(X_train, y_train)\n","    train_loss.append(mean_squared_error(y_train, rf.predict(X_train)))\n","    test_loss.append(mean_squared_error(y_test, rf.predict(X_test)))\n","\n","plt.figure(figsize=(10, 7))\n","plt.title(\"Изменение лосса в зависимости от количества деревьев\")\n","plt.grid()\n","plt.plot(train_loss, label=\"MSE_train\")\n","plt.plot(test_loss, label=\"MSE_test\")\n","plt.ylabel(\"MSE\")\n","plt.xlabel(\"# trees\")\n","plt.legend();"],"id":"ZhIv39slPz7O"},{"cell_type":"markdown","metadata":{"id":"gA--0Z5CPz7O"},"source":["Как и ожидалось, по достижении некоторого числа деревьев обе ошибки практически не изменяются, то есть переобучения при росте числа деревьев не происходит. Однако практика показывает, что при изменении какого-нибудь другого параметра на реальных данных переобучение может произойти: [пример 1](https://datascience.stackexchange.com/questions/1028/do-random-forest-overfit), [пример 2](https://mljar.com/blog/random-forest-overfitting/). Например, случайный лес с ограниченными по глубине деревьями может предсказывать более точно, чем лес без ограничений. В нашем же случае случайный лес, скорее, лишь страдает от регуляризации. Например, посмотрим на поведение модели при изменении максимальной глубины деревьев (поэксперементируйте с другими параметрами)."],"id":"gA--0Z5CPz7O"},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2023-05-10T11:52:43.149372Z","start_time":"2023-05-10T11:52:35.737047Z"},"id":"n3m1PPZRPz7O"},"outputs":[],"source":["max_depth = 50\n","train_loss = []\n","test_loss = []\n","\n","for i in trange(1, max_depth, 2):\n","    rf = RandomForestRegressor(n_estimators=20, max_depth=i, n_jobs=4)\n","    rf.fit(X_train, y_train)\n","    train_loss.append(mean_squared_error(y_train, rf.predict(X_train)))\n","    test_loss.append(mean_squared_error(y_test, rf.predict(X_test)))\n","\n","plt.figure(figsize=(10, 7))\n","plt.title(\"Изменение лосса в зависимости от глубины деревьев\")\n","plt.grid()\n","plt.plot(train_loss, label=\"MSE_train\")\n","plt.plot(test_loss, label=\"MSE_test\")\n","plt.ylabel(\"MSE\")\n","plt.xlabel(\"max_depth\")\n","plt.legend();"],"id":"n3m1PPZRPz7O"},{"cell_type":"markdown","metadata":{"id":"VWg75WDpPz7O"},"source":["Переобучение не наблюдается. Вообще же, как обычно, гиперпараметры случайного леса стоит подбирать на кросс-валидации."],"id":"VWg75WDpPz7O"},{"cell_type":"markdown","metadata":{"id":"qh2ShgNlPz7P"},"source":["### 3.2. Out-of-bag-ошибка\n","\n","Как мы обсудили выше, при построении случайного леса каждое дерево строится на бутстрапированной подвыборке, полученной из исходной обучающей выборки случайным набором с повторениями. Понятно, что некоторые наблюдения попадут в такую подвыборку несколько раз, а некоторые не войдут в неё вообще. Для каждого дерева мы можем рассмотреть объекты, которые не участвовали в обучении и использовать их для валидации.\n","\n","Усреднённая ошибка на неотобранных образцах по всему случайному лесу называется **out-of-bag-ошибкой**."],"id":"qh2ShgNlPz7P"},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2023-05-10T11:52:44.771132Z","start_time":"2023-05-10T11:52:43.162809Z"},"id":"htGKDKkiPz7P"},"outputs":[],"source":["# oob_score_ = R2 на невиденных наблюдениях для регрессии\n","# oob_score_ = Accuracy на невиденных наблюдениях для классификации\n","\n","rf = RandomForestRegressor(n_estimators=100, oob_score=True, n_jobs=4)\n","rf.fit(X_train, y_train)\n","rf.oob_score_"],"id":"htGKDKkiPz7P"},{"cell_type":"markdown","metadata":{"id":"qegGWbYjPz7P"},"source":["### 3.3. Важность признаков\n","Как и решающие деревья, случайный лес позволяет оценивать важность признаков. Важность признаков можно оценивать, например, по тому, насколько сильно уменьшается хаотичность при применении предикатов на основе этого признака."],"id":"qegGWbYjPz7P"},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2023-05-10T11:52:44.862097Z","start_time":"2023-05-10T11:52:44.778753Z"},"id":"MKMfu3BWPz7P"},"outputs":[],"source":["plt.figure(figsize=(10, 7))\n","plt.title(\"Feature importances\")\n","plt.bar(california[\"feature_names\"], rf.feature_importances_);"],"id":"MKMfu3BWPz7P"},{"cell_type":"markdown","metadata":{"id":"4RxvHH8EPz7P"},"source":["Будьте осторожны с сильно коррелирующими признаками. Посмотрим, что произойдёт с важностью, если добавить в выборку линейно зависимый признак."],"id":"4RxvHH8EPz7P"},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2023-05-10T11:52:44.865208Z","start_time":"2023-05-10T11:52:44.863222Z"},"id":"ny7IdZCQPz7Q"},"outputs":[],"source":["RM_mc = np.array((X_train.iloc[:, 5] * 2 + 3)).reshape(-1, 1)\n","X_train_new = np.hstack((X_train, RM_mc))"],"id":"ny7IdZCQPz7Q"},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2023-05-10T11:52:46.662664Z","start_time":"2023-05-10T11:52:44.866029Z"},"id":"u4Ha0JsePz7Q"},"outputs":[],"source":["rf.fit(X_train_new, y_train)"],"id":"u4Ha0JsePz7Q"},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2023-05-10T11:52:46.743774Z","start_time":"2023-05-10T11:52:46.664943Z"},"id":"8AzC500rPz7Q"},"outputs":[],"source":["plt.figure(figsize=(10, 7))\n","names = list(california[\"feature_names\"])\n","names.append(\"_AveOccup\")\n","plt.bar(names, rf.feature_importances_);"],"id":"8AzC500rPz7Q"},{"cell_type":"markdown","metadata":{"id":"KSFihZXqPz7Q"},"source":["Важности перераспределились между линейной зависимыми признаками `AveOccup` и `_AveOccup`. Не забывайте учитывать корреляции между признаками, если вы используете этот метод для отбора признаков. Также обратите внимание на предупреждение в документации `sklearn`: не стоит использовать этот метод и для признаков, в которых есть много уникальных значений (например, категориальные признаки с небольшим числом категорий)."],"id":"KSFihZXqPz7Q"},{"cell_type":"markdown","metadata":{"id":"XJf-Bu79Pz7Q"},"source":["## 4. Бонусная часть. Тестирование случайного леса на разных данных\n","Ниже представлены шаблоны для сравнения случайного леса и других моделей на данных разных типов. Проведите побольше экспериментов, используя разные модели и метрики. Попробуйте подобрать гиперпараметры случайного леса так, чтобы достичь какого-нибудь порога качества.\n","\n","**NB: Случайный лес может обучаться достаточно долго.**"],"id":"XJf-Bu79Pz7Q"},{"cell_type":"markdown","source":["### 4.1. Бинарная классификация на примере [Kaggle Predicting a Biological Response](https://www.kaggle.com/c/bioresponse/data?select=train.csv)"],"metadata":{"id":"sg5HFGsXPVuZ"},"id":"sg5HFGsXPVuZ"},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2023-05-10T11:52:46.746170Z","start_time":"2023-05-10T11:52:46.744028Z"},"id":"FCUcETbpPz7Q"},"outputs":[],"source":["# Загрузка данных\n","!wget  -O 'kaggle_response.csv' -q 'https://www.dropbox.com/s/uha70sej5ugcrur/_train_sem09.csv?dl=1'"],"id":"FCUcETbpPz7Q"},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2023-05-10T11:52:47.043457Z","start_time":"2023-05-10T11:52:46.747687Z"},"id":"6X3H8c-6Pz7Q"},"outputs":[],"source":["data = pd.read_csv(\"kaggle_response.csv\")\n","X = data.iloc[:, 1:].values\n","y = data.iloc[:, 0].values\n","X_train, X_test, y_train, y_test = train_test_split(\n","    X, y, test_size=0.3\n",")"],"id":"6X3H8c-6Pz7Q"},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2023-05-10T11:52:47.915340Z","start_time":"2023-05-10T11:52:47.044586Z"},"id":"ABv_VyZ3Pz7R"},"outputs":[],"source":["from sklearn.ensemble import RandomForestClassifier\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import roc_auc_score\n","\n","# TODO: обучите логистическую регрессию и случайный лес с дефолтными параметрами\n","# Сравните их AUC ROC на тестовой выборке\n","\n"],"id":"ABv_VyZ3Pz7R"},{"cell_type":"markdown","metadata":{"id":"6Gr5cyDWPz7R"},"source":["### 4.2. Изображения на примере [Fashion MNIST](https://github.com/zalandoresearch/fashion-mnist)"],"id":"6Gr5cyDWPz7R"},{"cell_type":"code","execution_count":null,"metadata":{"id":"sFO9ERaXPz7R"},"outputs":[],"source":["import tensorflow as tf\n","\n","(X_train, y_train), (X_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()"],"id":"sFO9ERaXPz7R"},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2023-05-10T11:52:48.583920Z","start_time":"2023-05-10T11:52:48.531764Z"},"id":"TZBVikBvPz7R"},"outputs":[],"source":["plt.imshow(X_train[0, :]);"],"id":"TZBVikBvPz7R"},{"cell_type":"code","execution_count":null,"metadata":{"ExecuteTime":{"end_time":"2023-05-10T11:53:32.049629Z","start_time":"2023-05-10T11:52:48.601595Z"},"id":"KzYMdwobPz7R"},"outputs":[],"source":["from sklearn.metrics import accuracy_score\n","from sklearn.neighbors import KNeighborsClassifier\n","\n","# TODO: обучите случайный лес и kNN с дефолтными параметрами\n","# Сравните их доли правильных ответов на тестовой выборке\n","\n"],"id":"KzYMdwobPz7R"},{"cell_type":"code","execution_count":null,"metadata":{"id":"vbxdXs8nPz7R"},"outputs":[],"source":[],"id":"vbxdXs8nPz7R"}],"metadata":{"kernelspec":{"display_name":"pyoadfe","language":"python","name":"pyoadfe"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}