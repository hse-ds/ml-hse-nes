{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huUAENWzqRjC"
      },
      "source": [
        "# Домашняя работа 3. Логистическая регрессия."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuxySVPXqRjJ"
      },
      "source": [
        "### Оценивание и штрафы\n",
        "\n",
        "Максимальная оценка — 10 баллов.\n",
        "\n",
        "Не списывайте, иначе всем участникам обнулим :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Av1a12xqqRjJ"
      },
      "source": [
        "Для удобства проверки самостоятельно посчитайте свою максимальную оценку (исходя из набора решенных задач) и укажите ниже.\n",
        "\n",
        "**Оценка: ...**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "386OpU3ZqRjK"
      },
      "outputs": [],
      "source": [
        "print('Всем удачи!👒')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SXGNPAxIqRjM"
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "pd.set_option('display.max_rows', 100)\n",
        "pd.set_option('display.max_columns', 100)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Логистическая регрессия"
      ],
      "metadata": {
        "id": "XXq13gt_fYsd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Модель логистической регрессии:\n",
        "  $$\n",
        "  \\hat y = \\sigma (Xw).\n",
        "  $$\n",
        "  Сигмоида меняется в пределах от 0 до 1 и имеет вид:\n",
        "  $$\n",
        "  \\sigma(x) = \\frac{1}{1+e^{-x}}.\n",
        "  $$\n",
        "\n",
        "  Функция потерь log-loss:\n",
        "  $$\n",
        "  L = -\\frac{1}{\\ell}\\sum_{i = 1}^{\\ell}(y_i\\log(\\hat y_i) + (1 - y_i)\\log(1 - \\hat y_i)),\n",
        "  $$\n",
        "  где $\\ell$ - количество объектов."
      ],
      "metadata": {
        "id": "kUH3O5v1fe5j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Градиентный спуск"
      ],
      "metadata": {
        "id": "kSGOXy37jhPv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Итеративный метод оптимизации, при котором вектор весов модели $\\mathbf{w}^{(t+1)}$ на шаге $t+1$ может быть выражен как:\n",
        "$$\n",
        "\\mathbf{w}^{(t+1)} = \\mathbf{w}^{(t)} - \\eta_t \\nabla L(\\mathbf{w}^{(t)}),\n",
        "$$\n",
        "где $\\eta_t$ - шаг обучения."
      ],
      "metadata": {
        "id": "kjjcRO7hjl5V"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPJLWbN2qRjM"
      },
      "source": [
        "## Часть 1. Логрег своими руками"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2wlZ_DDFqRjN"
      },
      "source": [
        "**Задание 1 (8 баллов)**. Реализуйте логистическую регрессию, обучаемую с помощью:\n",
        "- градиентного спуска **(4 балла)**\n",
        "\n",
        "- стохастического градиентного спуска **(4 балла)**\n",
        "\n",
        "Во всех пунктах необходимо соблюдать два условия:\n",
        "- Циклы можно использовать только для итераций градиентного спуска;\n",
        "- В качестве критерия останова необходимо использовать (одновременно):\n",
        "\n",
        "    - проверку на евклидову норму разности весов на двух соседних итерациях (например, меньше некоторого малого числа порядка $10^{-6}$), задаваемого параметром `tolerance`;\n",
        "    - достижение максимального числа итераций (например, 10000), задаваемого параметром `max_iter`.\n",
        "\n",
        "Чтобы проследить, что оптимизационный процесс действительно сходится, добавим атрибут класса `loss_history`. В нём после вызова метода `fit` должны содержаться значения функции потерь для всех итераций градиентного спуска, начиная с нулевой.\n",
        "\n",
        "Инициализировать веса можно случайным образом или нулевым вектором."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BNl4AHbyqRjO"
      },
      "outputs": [],
      "source": [
        "from sklearn.base import BaseEstimator\n",
        "\n",
        "class LogReg(BaseEstimator):\n",
        "    def __init__(self, gd_type: str = 'stochastic', tolerance: float = 1e-4,\n",
        "                 max_iter: int = 1000, eta: float = 1e-2,\n",
        "                 w0: np.array = None) -> None:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          gd_type: Type of gradient descent ('full' or 'stochastic').\n",
        "\n",
        "          tolerance: Threshold for stopping gradient descent.\n",
        "\n",
        "          max_iter: Maximum number of steps in gradient descent.\n",
        "\n",
        "          eta: Learning rate.\n",
        "\n",
        "          w0: Array of shape d (d — number of weights to optimize).\n",
        "              Initial weights.\n",
        "        \"\"\"\n",
        "        self.gd_type = gd_type\n",
        "        self.tolerance = tolerance\n",
        "        self.max_iter = max_iter\n",
        "        self.eta = eta\n",
        "        self.w0 = w0\n",
        "        self.w = None\n",
        "        self.loss_history = None\n",
        "\n",
        "    def fit(self, X: np.array, y: np.array) -> LogReg:\n",
        "        \"\"\"Fit the model on training data. Also, save value of loss after each iteration.\n",
        "\n",
        "        Args:\n",
        "          X: Training data.\n",
        "\n",
        "          y: Target.\n",
        "\n",
        "        Returns:\n",
        "          self: Fitted classsifier.\n",
        "        \"\"\"\n",
        "        self.loss_history = []\n",
        "        #╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
        "        return self\n",
        "\n",
        "\n",
        "    def predict_proba(self, X: np.array) -> np.array:\n",
        "        \"\"\"Calculate probability of positive and negative class for each observation.\n",
        "\n",
        "        Args:\n",
        "          X: Array of shape (n, d).\n",
        "             Data.\n",
        "\n",
        "        Returns:\n",
        "             Array of shape (n, 2).\n",
        "             Predicted probabilities.\n",
        "        \"\"\"\n",
        "        if self.w is None:\n",
        "            raise Exception('Not trained yet')\n",
        "        #╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
        "        pass\n",
        "\n",
        "\n",
        "    def predict(self, X: np.array) -> np.array:\n",
        "        \"\"\"Predict class for each observation.\n",
        "\n",
        "        Args:\n",
        "          X: Array of shape (n, d).\n",
        "             Data.\n",
        "\n",
        "        Returns:\n",
        "             Array of shape (n,).\n",
        "             Predicted class labels.\n",
        "        \"\"\"\n",
        "        if self.w is None:\n",
        "            raise Exception('Not trained yet')\n",
        "        #╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
        "        pass\n",
        "\n",
        "    def calc_gradient(self, X: np.array, y: np.array) -> np.array:\n",
        "        \"\"\"Calculate gradient of loss function after each iteration.\n",
        "\n",
        "        Args:\n",
        "          X: Array of shape (n, d), n can be equal to 1 if 'stochastic'.\n",
        "          y: Array of shape (n,).\n",
        "\n",
        "        Returns:\n",
        "          Array of shape (d,).\n",
        "          Gradient of loss function after current iteration.\n",
        "        \"\"\"\n",
        "        #╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
        "        pass\n",
        "\n",
        "    def calc_loss(self, X: np.array, y: np.array) -> float:\n",
        "        \"\"\"Calculate value of loss function after each iteration.\n",
        "\n",
        "        Args:\n",
        "          X: Array of shape (n, d).\n",
        "          y: Array of shape (n,).\n",
        "\n",
        "        Returns:\n",
        "          Value of loss function after current iteration.\n",
        "        \"\"\"\n",
        "        #╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DG6rOpEqRjP"
      },
      "source": [
        "Далее предполагается, что вы используете собственную реализацию логистической регрессии.\n",
        "Если с написанием класса возникли проблемы, используйте реализацию sklearn, чтобы не терять баллы за остальные задания."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7StGNunqRjP"
      },
      "source": [
        "Сгенерируем синтетические данные."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-iH4GcYqRjQ"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X, y = make_classification(\n",
        "    n_samples=10000, n_features=10, n_informative=5, n_redundant=5,\n",
        "    random_state=42)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVj4W4N4qRjQ"
      },
      "source": [
        "**Задание 2 (1 балл).** Обучите логистическую регрессию на синтетических данных. Нарисуйте кривую обучения."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z0jj-FYZqRjR"
      },
      "outputs": [],
      "source": [
        "### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfoTnVZDqRjR"
      },
      "source": [
        "На тестовой части посчитайте ROC-AUC, PR-AUC. Постройте ROC и PR кривые."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dHNqK-DSqRjS"
      },
      "outputs": [],
      "source": [
        "### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9Qcjpj7qRjS"
      },
      "source": [
        "**Задание 3 (1 балл).** Оцените ошибку ROC-AUC и PR-AUC вашей модели при помощи K-fold кросс валидации.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lDTHodjfqRjS"
      },
      "outputs": [],
      "source": [
        "### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "pyoadfe",
      "language": "python",
      "name": "pyoadfe"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}