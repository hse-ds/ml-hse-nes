{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNEp2xOs8HWGiMc2LNS2GSd"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Домашнее задание 2\n","\n","По курсу \"Машинное обучение\""],"metadata":{"id":"2Y1E1PJL0N18"}},{"cell_type":"markdown","source":["**Аннотация:**\n","\n","В этом задании вам нужно решить несколько задач по производным, линейной алгебре, градиентному спуску и линейной регрессии."],"metadata":{"id":"VJ3QOH4fXdoD"}},{"cell_type":"markdown","source":["## **Задача 1 (2 балла)**"],"metadata":{"id":"15ORFdwp05M8"}},{"cell_type":"markdown","source":["Рассмотрим алгоритм KNN для решения задачи регрессии. На лекциях мы говорили, что для нового объекта $u$ прогноз делается по формуле:\n","\n","\\begin{equation}\n","    \\hat{y}(u) = \\arg \\min_{c\\in\\mathcal{R}} \\sum_{j=1}^{k} w_j (y_u^{(j)}-c)^2\n","\\end{equation}\n","\n","где $c$ - 'усредненное' значение целевой переменной по соседям; $y_u^{(j)}$ - значение целевой переменной $j$-го соседа объекта $u$; $w_j$ - некоторый вес соседа; $\\hat{y}(u)$ - прогноз для объекта $u$. Покажите, что в явном виде прогноз можно записать так:\n","\n","\n","\\begin{equation}\n","    \\hat{y}(u) = \\frac{1}{\\sum_{j=1}^{k} w_j } \\sum_{j=1}^{k} w_j y_u^{(j)}.\n","\\end{equation}"],"metadata":{"id":"eHXeVl711bFD"}},{"cell_type":"markdown","source":["## **Задача 2 (2 балла)**"],"metadata":{"id":"s4KoX1TD05ck"}},{"cell_type":"markdown","source":["Рассмотрим функцию сигмоиды $\\sigma(x)$:\n","\n","\\begin{equation}\n","    \\sigma(x) = \\frac{1}{1+e^{-x}}.\n","\\end{equation}\n","\n","Покажите, что производная сигмоиды $\\sigma(x)'$ равна:\n","\n","\\begin{equation}\n","    \\sigma(x)' = \\sigma(x) (1-\\sigma(x)).\n","\\end{equation}"],"metadata":{"id":"cEd-lVMy1hgL"}},{"cell_type":"markdown","source":["## **Задача 3 (2 балла)**"],"metadata":{"id":"63P-A6cM05pi"}},{"cell_type":"markdown","source":["Пусть даны наблюдения:\n","\n","\\begin{array}{|c c|}\n"," \\hline\n"," x_1 & y \\\\ \\hline\n"," 0 & 0.1 \\\\ \\hline\n"," 0.5 & 0.9 \\\\ \\hline\n"," 1 & 2.1 \\\\ \\hline\n"," 1.5 & 2.9 \\\\ \\hline\n","\\end{array}\n","\n","Рассмотрим линейную регрессию:\n","\n","\\begin{equation}\n","    \\hat{y} = w_0 + w_1 x_1.\n","\\end{equation}\n","\n","Сделайте 3 итерации градиентного спуска, чтобы  найти значения весов $w_0, w_1$. Длину шага возьмите $\\eta=0.5$, а начальные значения $w_0=w_1=1$.\n"],"metadata":{"id":"LyHEW-HF1o0A"}},{"cell_type":"markdown","source":["## **Задача 4 (2 балла)**"],"metadata":{"id":"dDDDyu7r056O"}},{"cell_type":"markdown","source":["Найдите веса линейной регрессии из предыдущей задачи с помощью аналитической формулы."],"metadata":{"id":"zi9Xvuzy1vHM"}},{"cell_type":"markdown","source":["## **Задача 5 (2 балла)**"],"metadata":{"id":"fXsS8AKS05_0"}},{"cell_type":"markdown","source":["Рассмотрим линейную регрессию с $L_2$ регуляризацией. Докажите, что аналитическое решение задается формулой:\n","\n","\\begin{equation}\n","    w = (X^TX+ \\alpha I)^{-1}X^Ty,\n","\\end{equation}\n","\n","где $\\alpha$ - коэффициент регуляризации; $I$ - единичная матрица."],"metadata":{"id":"ptY2ypvf1ytQ"}}]}