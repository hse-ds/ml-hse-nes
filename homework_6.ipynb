{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IHgmxWG_7lnE"
   },
   "source": [
    "# Домашняя работа 6. Бустинг"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Максимальная оценка 10 баллов\n",
    "\n",
    "Ответы на вопросы пишите в комментариях или в markdown ячейках. Таким же образом обозначайте блоки кода для лучшей читаемости (например, \"Обучим бэггинг на логистических регрессиях : ### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ\").\n",
    "\n",
    "Для удобства проверки самостоятельно посчитайте свою максимальную оценку (исходя из набора решенных задач) и укажите ниже.\n",
    "\n",
    "**Оценка: ...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GOqjUI6igeLc"
   },
   "outputs": [],
   "source": [
    "# !pip install numpy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-tKaz0okgeLh"
   },
   "source": [
    "### Задание 1. Градиентный бустинг своими руками  (4 балла)\n",
    "\n",
    "Вам нужно реализовать упрощенный вариант градиентного бутсинга для задачи регресси. \n",
    "\n",
    "\n",
    "**Напоминание, как это работает:**\n",
    "\n",
    "Обозначим текущую композицию на $N-1$ шаге за $a_{N - 1}(x_i)$. Базовый алгоритм $b_N(x_i)$ обучается на ответах $-\\frac{\\partial L(y_i, z)}{\\partial z}\\Bigl|_{z = a_{N - 1}(x_i)}$, где $L(y_i, z)$ — значение функции потерь на объекте при правильном ответе $y_i$ и предсказании $z$. Композиция на следующем шаге получается так:\n",
    "\n",
    "$$\n",
    "a_N(x_i) = a_{N-1}(x_i) + \\nu\\gamma_Nb_N(x_i)\n",
    "$$\n",
    "\n",
    "Здесь $\\nu \\in [0, 1]$ — темп обучения (гиперпараметр), $\\gamma_N$ — оптимальный вес, настраиваемый на каждом шаге алгоритма в ходе решения оптимизационной задачи:\n",
    "\n",
    "$$\n",
    "\\gamma_N = \\mathrm{arg}\\min_\\gamma \\frac{1}{\\ell}\\sum\\limits_{i=1}^{\\ell}L\\left(y_i, a_{N - 1}(x_i) + \\gamma b_N(x_i)\\right)\n",
    "$$\n",
    "\n",
    "\n",
    "Заметьте, что в формуле выше нет $\\nu$. Этот гиперпараметр используется для сокращения длины шага, оптимального при составлении композиции $a_N$. Идея отклонения от оптимума должна быть вам уже знакома как способ борьбы с переобучением, когда мы специально форсим модель работать чуть хуже, чем могла бы, на текущем шаге, чтобы сохранить обобщающую способность и не подогнаться под тренировочную выборку (или под шум).\n",
    "\n",
    "С потерей в 0.5 балла можете принять $\\gamma_N = 1$ для каждого $N$. На полный балл необходимо реализовать нахождение оптимального $\\gamma_N$ на каждом шаге.\n",
    "\n",
    "В качестве функции потерь $L$ возьмите MSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве базовой модели можете использовать `DecisionTreeRegressor` из `sklearn`.\n",
    "Для решения оптимизационной задачки можно воспользоваться алгоритмами из любых библиотек, например, `scipy.optimize`, или найти оптимум перебором по сетке из некоторого разумного диапазона.\n",
    "\n",
    "Можно дописывать свои функции, если необходимо."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZB5Yt-LKgeLi"
   },
   "outputs": [],
   "source": [
    "class GradientBoosting:\n",
    "    def __init__(\n",
    "        self, \n",
    "        base_model_class: object = DecisionTreeRegressor,\n",
    "        base_model_params: dict = {'max_depth': None}, \n",
    "        n_estimators: int = 10,\n",
    "        learning_rate: float = 0.1\n",
    "    ):\n",
    "        \"\"\"\n",
    "        \n",
    "        Args:\n",
    "          base_model_class: Class of the base learner.\n",
    "\n",
    "          base_model_params: Hyperparameters of the base learner.\n",
    "          \n",
    "          n_estimators: Number of boosting stages.\n",
    "          \n",
    "          learning_rate: Value used to shrink contribution of each base learner to the model. \n",
    "          \n",
    "        \"\"\"\n",
    "        \n",
    "        self.base_model_class = base_model_class\n",
    "        self.base_model_params = base_model_params\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # list for optimal gammas at each iteration\n",
    "        self.gammas = []\n",
    "        \n",
    "        # list for base models\n",
    "        self.models = []\n",
    "        \n",
    "        \n",
    "    def find_optimal_gamma(self, \n",
    "                           y: np.array, \n",
    "                           old_predictions: np.array,\n",
    "                           new_predictions: np.array) -> float:\n",
    "        \"\"\"You may add arguments if it's necessary for your optimization algorithm.\n",
    "        \n",
    "        Args:\n",
    "          y: Target variable.\n",
    "\n",
    "          old_predictions: Prediction of the additive model at the previous stage.\n",
    "          \n",
    "          new_predictions: Prediction of the base learner at the current stage. \n",
    "          \n",
    "        Returns:\n",
    "          Optimal value for gamma.\n",
    "          \n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "    \n",
    "    def _fit_base_model(self, X: np.ndarray, y: np.array):\n",
    "        \"\"\"Train one base learner. \n",
    "        \n",
    "        Args:\n",
    "          X: Feature matrix\n",
    "          \n",
    "          y: Target variable.\n",
    "          \n",
    "          \n",
    "        Returns:\n",
    "          Fitted base learner.\n",
    "          \n",
    "        \"\"\"\n",
    "        pass\n",
    "    \n",
    "        \n",
    "    def fit(self, X: np.ndarray, y: np.array):\n",
    "        \"\"\"Train boosting (\"sum\" of base learners). \n",
    "        \n",
    "        Args:\n",
    "          X: Feature matrix\n",
    "          \n",
    "          y: Target variable.\n",
    "          \n",
    "          \n",
    "        Returns:\n",
    "          Fitted boosting.\n",
    "          \n",
    "        \"\"\"\n",
    "        pass\n",
    "       \n",
    "        \n",
    "    def predict(self, X: np.ndarray):\n",
    "        \"\"\"Make prediction of fitted boosting. \n",
    "        \n",
    "        Args:\n",
    "          X: Feature matrix\n",
    "\n",
    "\n",
    "        Returns:\n",
    "          Prediction of fitted boosting.\n",
    "          \n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверьте вашу реализацию на бостонском датасете. Подберите оптимальные гиперпараметры, чтобы победить RandomForestRegressor (не меняйте параметры сида)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = fetch_california_housing()\n",
    "X = housing.data[:5000]\n",
    "y = housing.target[:5000]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf = RandomForestRegressor(max_features=4, n_estimators=640, random_state=19052019)\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "mean_squared_error(y_test, rf.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2. Сравнение подходов (3 балла)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Скачайте данные о выдаче кредитов. Это данные с kaggle, целевая переменная `y` показывает, вернуло ли кредит физическое лицо."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget  -O 'bank_data.csv' -q 'https://www.dropbox.com/s/uy27mctxo0gbuof/bank_data.csv?dl=0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('bank_data.csv')\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Решите задачу предсказания возвращения кредита методами, перечисленными ниже:\n",
    "\n",
    "- Случайный лес\n",
    "- Бэггинг на деревьях (поставьте для базовых деревьев min_samples_leaf=1)\n",
    "- Бэггинг, у которого базовой моделью является бустинг с большим числом деревьев (> 100)\n",
    "- Бэггинг на логистических регрессиях\n",
    "\n",
    "Используйте логистическую регрессию, случайный лес, `GradientBoostingClassifier` и `BaggingClassifier` из `sklearn`.\n",
    "\n",
    "1) Какая из моделей имеет лучшее качество? С чем это связано?\n",
    "\n",
    "2) Какая из моделей сильнее всего переобучается?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 3. Современные бустинги (3 балла)\n",
    "\n",
    "Сравните на этих данных любую из трёх популярных имплементаций градиентного бустинга (xgboost, lightgbm, catboost). Подберите основные гиперпараметры (число деревьев, длина шага, глубина дерева/число листьев). Получилось ли круче, чем с моделями выше?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### ╰( ͡° ͜ʖ ͡° )つ──☆*:・ﾟ"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "hw8-boosting-clustering.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "pyoadfe",
   "language": "python",
   "name": "pyoadfe"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
